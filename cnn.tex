\section{Свёрточные нейронные сети}


Свёрточные нейронные сети (Convolutional Neural Network, CNN)- незаменимый инструмент для решения подавляющего большинства задач компьютерного зрения. 
На сегодняшний день существует множество различных архитектур, нацеленных на решение той или иной задачи. 
Очень часто свёрточные сети применяют как бекбон - сеть, которая предобучена извлекать признаки из изображения. К получившейся 
на выходе из такой сети карте признаков можно применять дополнительные преобразования для решения конкретной задачи. 

В рассмотренных нами подходах к решению задачи выявления заметных объектов на изображении свёрточные нейронные сети 
применяются и как самостоятельная модель(?), и в качестве бекбона. Однако, основные составные части свёрточной сети 
остаются практически неизменными. В этой главе будут описаны основные операции, характерные для архитектуры свёрточной нейронный сети,
а также рассмотрены архитектуры ResNet-50\cite{ResNet} и Efficient-Net b0 \cite{Efficientnet}, которые будут использованы в качестве 
бекбонов в экспериментах с моделью BBS-Net \cite{BBS}

//TODO: два предложение чё такое карта признаков

\subsection{Основные операции}

Основные операции в свёрточнах нейронных сетях - это идущие последовательно свёртка, активация и пулинг. Блоки из свёртки и операции
обычно повторяются несколько раз перед применением пулинга \cite{ResNet}. Рассмотрим эти операции подробнее.
// TODO: слой или операция свёртки и остальные? 

\subsubsection{Операция свёртки}

Свёрточный слой - основной строительный блок свёрточной нейронной сети. Каждый слой свёртки содержит несколько ядер установленного размера,
обычно $3 \time 3$ или $5 \time 5$, каждое из которых применяется ко входной карте активации и с некотором шагом проходит по всей карте,
перемножая параметры ядра с параметрами входное карты вдоль всех каналов, а затем складывая их друг с другом и записывая в новую карту признаков.

// TODO: картинка!

Обычно у свёртки рецептивное поле, то есть группа чисел в карте признаков, попадающая под действие свёртки,
соответствует форме и размеру ядра, например квадрату $3 \time 3$. Однако, существует более общий вид,
называемый расширенной свёрткой(Dilated Convolutions)\cite{Dilated}. У таких свёрток рецептивное поле контролируется параметром скорости
расширения (dilation rate). Так, для стандартной свёртки он равен 1. Если параметр равен 2, то рецептивное поле увеличивается
до размера $7 \time 7$, как показано на изображении \ref{fig:dilated}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{dilated}
    \caption{Пример расшеренной свёртки.     (a) параметр dilation rate равен 1, что соответствует обычной свёртке. (b) параметр dilation rate равен 2}
    \label{fig:dilated}
\end{figure}


\subsubsection{Пулинг}

Операция пулинга(Pooling), или операция субдискретизации, представляет собой операцию уменьшения размерности входящей карты признаков 
путём применения некоторой стратегии. Самой распространённой стратегией является стратегия максимально пулинга(Max Pooling),
при которой из группы соседних чисел, например внутри квадрата $2 \times 2$, выбирается максимальное и записывается в новую
карту признаков 

//TODO картинку!

Операция пулинга может быть записана следующим образом:

\begin{equation}
    f_{x,y}(S) = \max_{a,b=0}^{s}S_{sX+a, sY+b}
\end{equation}

где $S$ - карта активации, $s$ - размер окна.

\subsubsection{Функция активации ReLU}

Обычно, после применения свёртки, следующим шагом идёт применение функции активации для добавления нелинейности. Таким образом,
несколько последовательных свёрток, являясь линейными операциями, не будут эквивалентны одной свёртке, а будут описывать действительно 
разные признаки объекта. Одной из самых популярных функций активаций стала функция ReLU(Rectified Linear Unit), которая определяется
как.

\begin{equation}
    f(x) = max(0,x)
\end{equation}

//TODO: картиночка с функцией


\subsubsection{Пакетная нормализация}

Пакетная нормализация (Batch Normalization) - метод, позволяющий сделать 
нейронную сеть более устойчивой и производительной. Он был предложен в 
работе \cite{Batch-Norm}. Идея метода в предварительной нормализации
и центрировании карт признаков, подающихся на определённые слои сети.
Кроме того, слой также имеет обучамые параметры масштабирования
и сдвига.

Пакетная нормализация может быть выражена следующим образом.
На вход слою пакетной нормализации BN поступает пакет объектов-признаков:
$B = (x_1 \dots x_m)$. На выходе каждому объекту $x_i, i \in \{1 \dots m\}$
ставится в соответствие значение $y_i$:
\begin{equation}
    y_i = BN_{\gamma, \beta}(x_i) = \gamma \hat{x_i} + \beta
\end{equation}
где $\gamma$ и $\beta$ - обучаемые параметры масштабирования и сдвига соответственно,
а 

\begin{equation}
    x_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \text{ где} \\
\end{equation}
где $\mu_B$ - выборочной среднее по пакету: 
\begin{equation}
    \mu_B = \frac{1}{m}\sum_{i=1}^{m}x_i 
\end{equation}
а $\sigma_B^2$ - выборочная дисперсия:
\begin{equation}
    \sigma_B^2 = \frac{1}{m}(x_i - \mu_B)^2
\end{equation}


\subsection{Бекбоны}

Чтобы 
.. Нужно преобразовать изображение в некоторую матрицу, хранящую признаки. На основе этих 
признаков мы можем решать задачи классификации, распознавая что изображено на изображении, детекции и сегментации, делая вывод о местоположении этого объекта,
а также огромное число других задач компьютерного зрения, в том числе и задачу SOD.

Именно для извлечения этих признаков и используются предобученные свёрточные сети, называемые бекбонами. Они позволяют 
абстрагироваться от построения процесса извлечения признако и сосредоточиться на решении конкрентной задачи.
А главное - использование бекбонов существенно сокращает время тренировки и необходимые для неё мощности,
ведь веса бекбона уже обучены и могут быть заморожены во время обучения, а обучать можно только специальные добавленные слои.

// ТУДУ подредактировать

В данной работе мы будем использовать свёрточные сети ResNet-50\cite{ResNet} и EfficientNet b-0\cite{Efficientnet} 
в качестве бекбона для сети BBS-Net\cite{BBS}. Ниже рассмотрим основные особенности этих сетей.

\subsubsection{ResNet-50}

Архитектура ResNet была предложена в работе \cite{ResNet} и произвела настоящую революцию 
в изучении и проектировнии свёрточных нейронных сетей. Главная отличительная особенность данной архитектуры -
резидьюал конекшны //ТУДУ . - операции вида 

// ТУДУ y = x + f(x)
% y = F(x, {Wi}) + x. 


применяющиеся несколько раз. а вместо функции стоит целый блок из свёрток - //ТУДУ картинка.
Подобное улучшение позволило обучать гораздо более глубокие сети. 
В работе резнет были представлены резнет59 и 101 с такой-то глубиной, когда как предшествующие модели вгг имели столько-то 


\subsubsection{Efficient-Net}

