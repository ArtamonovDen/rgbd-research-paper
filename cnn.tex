\section{Свёрточные нейронные сети}


Свёрточные нейронные сети (Convolutional Neural Network, CNN)- незаменимый инструмент для решения подавляющего большинства задач компьютерного зрения. 
На сегодняшний день существует множество различных архитектур, нацеленных на решение той или иной задачи. 
Очень часто свёрточные сети применяют как бекбон - сеть, которая предобучена извлекать признаки из изображения. К получившейся 
на выходе из такой сети карте признаков можно применять дополнительные преобразования для решения конкретной задачи. 

В рассмотренных нами подходах к решению задачи выявления заметных объектов на изображении свёрточные нейронные сети 
применяются и как самостоятельная модель(?), и в качестве бекбона. Однако, основные составные части свёрточной сети 
остаются практически неизменными. В этой главе будут описаны основные операции, характерные для архитектуры свёрточной нейронный сети,
а также рассмотрены архитектуры ResNet-50\cite{ResNet} и Efficient-Net b0 \cite{Efficientnet}, которые будут использованы в качестве 
бекбонов.

//TODO: два предложение чё такое карта признаков

Основные операции в свёрточнах нейронных сетях - это идущие последовательно свёртка, активация и пулинг. Блоки из свёртки и операции
обычно повторяются несколько раз перед применением пулинга \cite{ResNet}. Рассмотрим эти операции подробнее.
// TODO: слой или операция свёртки и остальные? 


\subsection{Операция свёртки}

Свёрточный слой - основной строительный блок свёрточной нейронной сети. Каждый слой свёртки содержит несколько ядер установленного размера,
обычно $3 \time 3$ или $5 \time 5$, каждое из которых применяется ко входной карте активации и с некотором шагом проходит по всей карте,
перемножая параметры ядра с параметрами входное карты вдоль всех каналов, а затем складывая их друг с другом и записывая в новую карту признаков.

// TODO: картинка!

Обычно у свёртки рецептивное поле, то есть группа чисел в карте признаков, попадающая под действие свёртки,
соответствует форме и размеру ядра, например квадрату $3 \time 3$. Однако, существует более общий вид,
называемый расширенной свёрткой(Dilated Convolutions)\cite{Dilated}. У таких свёрток рецептивное поле контролируется параметром скорости
расширения (dilation rate). Так, для стандартной свёртки он равен 1. Если параметр равен 2, то рецептивное поле увеличивается
до размера $7 \time 7$, как показано на изображении \ref{fig:dilated}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{dilated}
    \caption{Пример расшеренной свёртки.     (a) параметр dilation rate равен 1, что соответствует обычной свёртке. (b) параметр dilation rate равен 2}
    \label{fig:dilated}
\end{figure}


\subsection{Пулинг}

Операция пулинга(Pooling), или операция субдискретизации, представляет собой операцию уменьшения размерности входящей карты признаков 
путём применения некоторой стратегии. Самой распространённой стратегией является стратегия максимально пулинга(Max Pooling),
при которой из группы соседних чисел, например внутри квадрата $2 \times 2$, выбирается максимальное и записывается в новую
карту признаков 

//TODO картинку!

Операция пулинга может быть записана следующим образом:

\begin{equation}
    f_{x,y}(S) = \max_{a,b=0}^{s}S_{sX+a, sY+b}
\end{equation}

где $S$ - карта активации, $s$ - размер окна.

\subsection{Функция активации ReLU}

Обычно, после применения свёртки, следующим шагом идёт применение функции активации для добавления нелинейности. Таким образом,
несколько последовательных свёрток, являясь линейными операциями, не будут эквивалентны одной свёртке, а будут описывать действительно 
разные признаки объекта. Одной из самых популярных функций активаций стала функция ReLU(Rectified Linear Unit), которая определяется
как.

\begin{equation}
    f(x) = max(0,x)
\end{equation}

//TODO: картиночка с функцией


\subsection{Пакетная нормализация}

Пакетная нормализация (Batch Normalization) - метод, позволяющий сделать 
нейронную сеть более устойчивой и производительной. Он был предложен в 
работе \cite{Batch-Norm}. Идея метода в предварительной нормализации
и центрировании карт признаков, подающихся на определённые слои сети.
Кроме того, слой также имеет обучамые параметры масштабирования
и сдвига.

Пакетная нормализация может быть выражена следующим образом.
На вход слою пакетной нормализации BN поступает пакет объектов-признаков:
$B = (x_1 \dots x_m)$. На выходе каждому объекту $x_i, i \in \{1 \dots m\}$
ставится в соответствие значение $y_i$:
\begin{equation}
    y_i = BN_{\gamma, \beta}(x_i) = \gamma \hat{x_i} + \beta
\end{equation}
где $\gamma$ и $\beta$ - обучаемые параметры масштабирования и сдвига соответственно,
а 

\begin{equation}
    x_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \text{ где} \\
\end{equation}
где $\mu_B$ - выборочной среднее по пакету: 
\begin{equation}
    \mu_B = \frac{1}{m}\sum_{i=1}^{m}x_i 
\end{equation}
а $\sigma_B^2$ - выборочная дисперсия:
\begin{equation}
    \sigma_B^2 = \frac{1}{m}(x_i - \mu_B)^2
\end{equation}


