\section{SOD}

Здесь накинуть побольше про SOD и про существующие подходы. Прямо пройтись по истории из статейки.

Потом как-то плавно перейти к ббс-у и сказать, почему мы берём её. Ну типа она современная(актуальная), свёрточная и без хитрых энкодеров.

Мы хотим её немножко прокачать - сделать поменьше и возможно улучшить точность.

Чтобы сделать её поменьше мы используем efficient net. А чтобы улучшить качество экспериментируем  с функциями потерь.



-----
Задача SOD 



//TODO встпление адекватное

Решая задачу SOD исследователи дошли до использования карты глубины как способу получить пространственную информацию об объектах на изображении.

За это время были предложены различные подходы к решению этой задачи: от использования [традиционный модеои, гаусс блабла] до использования глубоких свёрточных сетей[несколько ссылок на всякие сетки] и атоэнкодеров[ссылка на UNet]

Разделяют традиционные и глубокие модели.


Традиционные алгоритмы, не использующие нейронные сети, основаны на извлечении и анализе заранее определенных признаков RGB изображения и соответствующей ему карты глубины.
Например, такими признаками может быть контрастность, цвет или границы определённой части изображения. 
В работе \cite{Depth-really-Matters} была предложена модель, использующая геометрические признаки поверностей объектов в помещении и глубину. 
[TODO: можно что-то ещё, но правда непонятно, тчо они делали]
А в работе \cite{Depth-View-of-Saliency} для расчета заметности объекта используют цветовой контраст и вычисление нормали к поверхности\cite{Surface-Normal} для каждого пикселя на изображении.

// Дальше чё-то про локал/глобал, но это непонятно. Наверное просто ещё работ докинуть: Therefore, some algorithms, such as spatial
prior [35], global contrast [65], and background prior [66],

[Можно добавить картинку из https://arxiv.org/pdf/2008.00230.pdf и краткое описание моделек]


С развитием глубоких нейронных сетей на смену традиционным подходам к решению задачи SOD пришли модели, основанные на различных архитектурах нейронных сетей.
Так, в работе \cite{RGBD-SOD-Deep-Fusion} былл предложен метод, который предполагали применение свёрточной нейронной сети (CNN) к предварительно подготовленным признакам, а не давали модели самой извлекать признаки 
с сырого изображения. Признаки сначала извлекали с помощью традиционных методов: выделение фона и цветовых контрастов, 
получение информации о положении объекта в пространстве. Такие извлечённые признаки образовывали каналы изображения, которые и передавались в нейронную сеть, которая 
и выявляла самый заметный регион на картинке. 

// TODO можно картинку с  \cite{RGBD-SOD-Deep-Fusion}

Последующие предложенные модели, первой из которых была работа \cite{CNNs-Based} использовали более современный end-to-end подход к использованию CNN, предоставляя нейронной сети извлекать 
и высокоуровневые, и низкоуровневые признаки с изображений самостоятельно и без предварительных ручных шагов.

Various architectures have been designed to effectively
integrate the multi-scale features. 

For example, Liu et al. [25]
obtained saliency map outputs from each side-out features
by feeding a four-channel RGB-D image into a single backbone (single stream).


Chen et al. [22] leveraged two independent networks to extract RGB and depth features respectively, and then combined them in a progressive merging
way (double stream). 



Вопрос о том, как использовать пространственную информацию, полученную с помощью карты глубины, вместе с RGB изображением породил целый ряд
различных подходов в объединении признаков с обоих изображений (fusion strategies). 