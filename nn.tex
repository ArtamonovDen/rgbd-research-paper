\section{Что-то про нейронки}

Современные подходы к решению задачи СОД какой-то глагол/активно юзают модели основанные на 
свёрточных нейронках. В этой главе? дадим обзор чё такое  свёрточные сети, опишем операции?. 

Расскажем про архитектуры, которые в данной работе мы будем юзать в качестве фича экстракторов.

Главным кирпичок является - свёртка / операция свёртки.
// вот тут видмо её надо определеить и пример мб привести

// Дальше про виды свёрток. Мол кроме традиционной есть ещё такая и такая  

// Рука об руку со свёрткой иде пулинг. Определение. Бывает пулинг не простой а глобальный

// Про батч норму , релу и резидьал? 


// TODO

а может 
CNN       -- как фича экткстратктор и основа
 - операции
 - экстракторы
    - RESNET
    - Effnet
BBS
А в экспериментах где-нибудь написать про лоссы и метрики.

//

\subsection{Свёрточные Нейронные Сети}

Я хер его знает с чего начать

Свёрточные сети стали SOta подходом при работе с изображениями / при решении различных задачах, связанных с изображениями.

Так получилось потому что каскад свёрток оч хорошо извлекает разные признаки и понимает картиночку. ?? 




Общее про свёртки.

Про всякие деконвы тож можно
// TODO побольше про dilaion rate: https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d


//TODO: про апсэмплинг

//TODO: про релу? 

// операцию глобального пулинга

Потом скажем, что в нужной нам модели встраиваются готовые предобученные модели. И дальше рассказываем про них
\subsubsection{Архитектура ResNet}
\subsubsection{Архитектура Efficient Net}  --
    -- можно бекбоны прям так сильно и на описывать. Просто в двух предложениях сказать какая разница.

\subsection{Функции потерь}


// TODO мб про лоссы и метркии сразу писать с точки зрения задачи СОД с обозначениями S и G?

Для тренировки нейронных сетей используют различные алгоритмы, основанные на стохастическом градиентном спуске,
для максимизации или минимизации некоторой целевой функции. В качестве целевой функции используют различные функции потерь, 
которые нужно минимизировать. Вид этих функций зависит от типа решаемой задачи, а выбор подходящей функции напрямую влияет на 
качество обучаемой модели. 

Для решения задачи выявления самых заметных объектов на изображении 
во многих работах [неплохо бы ссылочки], в том числе и в исследуемой нами \cite{BBS}, применяется функция бинарной перекрёстной энтропии.

В результате работы модели мы ожидаем получить бинарной изображение, которое содержит выделение наиболее заметных областей. 
Другими словами, мы хотим классифицировать каждый пиксель на изображении и отнести его либо к группе "выделяющихся", либо "не выделяющихся". То есть,
решить задачу сегментации - классификации пикселей на изображении. Таким образом, использование бинарной перекрестной энтропии 
в качестве функции потерь вполне естественно.

Однако, за последние годы для улучшения результатов задачи сегментации изображений были предложены и другие функции потерь, подходящие для 
этой задачи \cite{Loss-Functions}. Среди них : 


// TODO: русифицировать заголовочки

\begin{itemize}
    \item Dice Loss \cite{Dice-Loss}
    \item Jaccard Loss(Intersection over Union) \cite{IoU-Loss}
    \item Focal Loss \cite{Focal-Loss}
\end{itemize}

Мы проведём эксперименты с использованием каждой из функций и сравним результаты, полученные при использовании бинарной перекрёстной энтропии.
Остановимся подробнее на каждой из приведённых функций.

// TODO ввести обозначения y и ypred
// TODO поменять $\hat{y} $ на $\hat{p}$

\subsubsection{Бинарная Перекрёстная Энтропия}

Функция бинарной перекрёстной энтропии(binary cross entropy) является частным случаем функции перекрёстной энтропии - одной из самых популярных 
функций потерь для задач классификации и, как следствие, сегментации - попиксельной классификации.

Перекрёстная энтропия \cite{CE} определяется следующим образом: 
\begin{equation}
    L_{CE} = -\sum_{i=1}^{C}y_c\log{\hat{y_c}}
\end{equation}
где $y$ - истинный класс объекта, $y_c$ - предсказанный класс, а $C$ - количество 
всех классов.

В случае предсказания только двух классов, то есть при $C=2$, 
и возможных значениях меток $y \in \{0,1\}$
перекрёстная энтропия превращается в бинарную:

\begin{equation}
    L_{CE} = -\sum_{i=1}^{2}y_c\log{\hat{y_c}} = 
    -(y\log{\hat{y}} + (1-y)\log{1-\hat{y}})
\end{equation}



\subsubsection{Dice Loss} 

Коэффициент Дайса широко используется в компьютерном зрении для оценки сходства 
между двумя изображениями. В работе \cite{Dice} была предложена адаптация коэффициента 
для использования его в качестве функции потерь 

\begin{equation}
    L_{DL}(y, \hat{p} ) = 1 - \frac{2y\hat{p} + 1}{y + \hat{p} + 1}
\end{equation}

//TODO - единички поменять на параметр смуз? 


\subsubsection{IoU Loss}

Коэффициент Жаккара, использующийся как мера 
сходства и различия двух множеств \cite{IoU-Loss}:

\begin{equation}
    J(A,B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A + B - A \cap B|}
\end{equation}

Это отношение также называют пересечением над объединением 
(Intersection over Union, IoU). В контексте задач сегментации и детекции, 
оно позволяет определить, насколько 
хорошо предсказанная моделью область изображения перекрывает
область искомую.

Коэффициент Жаккара легко может быть преобразован в функцию потерь\cite{IoU-Loss-2}:

\begin{equation}
    L_{IoU} = 1- \frac{|A \cap B|}{|A \cup B|} = 1 - \frac{|A \cap B|}{|A + B - A \cap B|}
\end{equation}


\subsubsection{Focal Loss}

Фокальная функция потерь Лол, представленный в работе \cite{Focal-Loss}, является вариацией бинарной перекрёстной энтропии.
Отличительной чертой этой функции является возможность снижать вклад простых примеров и фокусироваться на примерах, более сложных
для обучения. //TODO чего?
В частности, функция хорошо показывает себя в случае сильно несбалансированных классов.
Например, в случае когда сегментированная область гораздо меньше изображения.

Как выводится фокал лосс из кросс энтропии //// ТУдУ

\begin{equation}
    CE=\begin{cases}
        -\log{p} & \text{if }y=1\\
        -\log{1-p} & \text{if }y=0
     \end{cases}
\end{equation}

Для удобства введём новое обозначение оценочной вероятности класса:

\begin{equation}
    p_t=\begin{cases}
        p & \text{if }y=1\\
        1-p & \text{if }y=0
     \end{cases}
\end{equation}

Тогда функцию перекрёстной энтропии можно переписать как

\begin{equation}
    CE(p,y) = CE(p_t) = -\log{p_t}
\end{equation}

Фокальная функция потерь имеет вид

\begin{equation}
    FL(p_t) = -\alpha_t(1-p_t)^{\gamma}\log{p_t}
\end{equation}
 

Focal Loss proposes to down-weight easy examples and focus
training on hard negatives using a modulating factor,

//TODO: тут он с помощью коэффициента навидываем не просто "сложным", 
а hard negatives . Как бы это описать 

\subsection{Метрики Качества}

//TODO / Сразу про пиксели и СОД? или нет? 

Чтобы оценить качество получившийся модели используют различные метрики. Мы рассмотрим 
две наиболее широко используемых в задачах SOD метрики: среднюю абсолютную ошибку (Mean Absolute Error, MAE) и F-меру(F-measure).

Средняя абсолютная ошибка - это сумма разностей модулей соответствующих значений пикселей предсказанного и истинного изображения.// TODO?
\begin{equation}
    mae = \frac{1}{N}\sum_{i=1}^N|y_i - \hat{p_i}|
\end{equation}

F-мера //TODO


\begin{equation}
    F_1 = 2\frac{precision \times recall}{precision + recall}
\end{equation}
где $precision$ //TODO